[
["index.html", "Matrix Algebra From A Statistician’s Perspective Preface", " Matrix Algebra From A Statistician’s Perspective Caleb Jin|金时强 2021-02-15 Preface This is my notes of the book titled “Matrix Algebra From A Statistician’s Perspective” by David A. Harville. Since Chapter 1-Mattices and Chapter 2-Submatrices and Partitioned Matrices are very simple and basic, I omit both parts. "],
["matrices.html", "Chapter 1 Matrices", " Chapter 1 Matrices Omit "],
["submatrices-and-partitioned-matrices.html", "Chapter 2 Submatrices and Partitioned Matrices", " Chapter 2 Submatrices and Partitioned Matrices Omit "],
["linear-dependence-and-independence.html", "Chapter 3 Linear Dependence and Independence", " Chapter 3 Linear Dependence and Independence Definition 3.1 A nonempty finite set \\(\\{{\\bf A}_1,{\\bf A}_2,\\ldots,{\\bf A}_k\\}\\) of \\(m\\times n\\) matrices is said to be linearly independent if there exist scalars \\(x_1,x_2,\\ldots,x_k\\), not all zero, such that \\[ x_1{\\bf A}_1 + x_2{\\bf A}_2 + \\ldots + x_k{\\bf A}_k = 0. \\] If no such scalars exist, the set is called linearly independent. The empty set is considerted to be linear independent. Lemma 3.2.4 Let \\({\\bf A}_1,{\\bf A}_2,\\ldots,{\\bf A}_k\\) represent \\(m\\times n\\) matrices and are linearly independent. Further, for \\(j = 1,\\ldots,r\\), let \\({\\bf x}_j = (x_{1j}, x_{2j}, \\ldots, x_{kj})^{{\\top}}\\), and let \\[ {\\bf C}_j = x_{1j}{\\bf A}_1 + x_{2j}{\\bf A}_2 + \\ldots + x_{kj}{\\bf A}_k. \\] Firstly, If \\({\\bf x}_1, {\\bf x}_2, \\ldots, {\\bf x}_r\\) are linearly independent, then \\({\\bf C}_1,{\\bf C}_2,\\ldots,{\\bf C}_r\\) are linearly independent. Secondly, If \\({\\bf x}_1, {\\bf x}_2, \\ldots, {\\bf x}_r\\) are linearly dependent, then \\({\\bf C}_1,{\\bf C}_2,\\ldots,{\\bf C}_r\\) are linearly dependent. Observe that, for any scalars \\(y_1,y_2,\\ldots,y_r\\), and \\[ \\sum_{j=1}^{r}y_j{\\bf x}_j = \\left(\\sum_{j=1}^{r}y_jx_{1j}, \\sum_{j=1}^{r}y_jx_{2j}, \\ldots, \\sum_{j=1}^{r}y_jx_{kj}\\right)^{{\\top}} = (z_1,z_2,\\ldots,z_k)^{{\\top}}. \\] \\[\\begin{eqnarray} \\sum_{j=1}^{r}y_j{\\bf C}_j &amp;=&amp; \\left(\\sum_{j=1}^{r}y_jx_{1j}\\right){\\bf A}_1 + \\left(\\sum_{j=1}^{r}y_jx_{2j}\\right){\\bf A}_2 + \\ldots + \\left(\\sum_{j=1}^{r}y_jx_{kj}\\right){\\bf A}_k \\\\ &amp;=&amp; z_1{\\bf A}_1 + z_2{\\bf A}_2 + \\ldots + z_k{\\bf A}_k. \\tag{3.1} \\end{eqnarray}\\] Firstly, If there exits \\(y_1,y_2,\\ldots,y_r\\) such that \\(\\sum_{j=1}^{r}y_j{\\bf C}_j={\\boldsymbol 0}\\), it implies \\(z_1,z_2,\\ldots,z_k\\) are 0s in Eq. (3.1), because \\({\\bf A}_1,{\\bf A}_2,\\ldots,{\\bf A}_k\\) are linearly independent. Hence \\(\\sum_{j=1}^{r}y_j{\\bf x}_j = 0\\), implying (in light of the linear independence of \\({\\bf x}_1, {\\bf x}_2, \\ldots, {\\bf x}_r\\)) that \\(y_1=y_2=\\ldots=y_r=0\\). Secondly, suppose \\({\\bf x}_1, {\\bf x}_2, \\ldots, {\\bf x}_r\\) are linearly dependent, then there exists \\(y_1,y_2,\\ldots,y_r\\), not all of which are zero, such that \\(\\sum_{j=1}^{r}y_j{\\bf x}_j={\\boldsymbol 0}\\), implying all \\(z_1,z_2,\\ldots,z_k\\) are zero. Hence (in the light of Eq. (3.1)) \\(\\sum_{j=1}^{r}y_j{\\bf C}_j=0\\). Thus, \\({\\bf C}_1,{\\bf C}_2,\\ldots,{\\bf C}_r\\) are linearly independent. "]
]
